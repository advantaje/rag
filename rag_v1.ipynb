{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cheat Sheet - Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22418c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for RAG pipeline\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Union\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import InMemoryVectorStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from utils.openai_tools import OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6051798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Methodology: semantic\n",
      "LLM Models: ['gpt-4o']\n",
      "Number of runs per question: 3\n",
      "DOCX folder: ./model_docs\n",
      "Questions file: ./example_questions.txt\n",
      "Custom prompt configured: 203 characters\n",
      "Output directory: ./outputs/semantic\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters for RAG experiment\n",
    "METHODOLOGY_ID = \"semantic\"\n",
    "\n",
    "LLM_MODELS = [\"gpt-4o\"]\n",
    "\n",
    "N_RUNS = 3\n",
    "\n",
    "DOCX_FOLDER_PATH = \"./model_docs\"\n",
    "\n",
    "QUESTIONS_FILE_PATH = \"./example_questions.txt\"\n",
    "\n",
    "CUSTOM_RAG_PROMPT = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "OUTPUT_DIR = f\"./outputs/{METHODOLOGY_ID}\"\n",
    "EMBEDDING_CACHE_DIR = f\"./cache/{METHODOLOGY_ID}\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(EMBEDDING_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Methodology: {METHODOLOGY_ID}\")\n",
    "print(f\"LLM Models: {LLM_MODELS}\")\n",
    "print(f\"Number of runs per question: {N_RUNS}\")\n",
    "print(f\"DOCX folder: {DOCX_FOLDER_PATH}\")\n",
    "print(f\"Questions file: {QUESTIONS_FILE_PATH}\")\n",
    "print(f\"Custom prompt configured: {len(CUSTOM_RAG_PROMPT)} characters\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce9dc0f",
   "metadata": {},
   "source": [
    "## Document Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2wkc8ra5psv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for formatting documents in LCEL chains\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format retrieved documents by joining their content with double newlines.\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Token counting function (placeholder - implement with your preferred tokenizer)\n",
    "def get_token_count(text):\n",
    "    \"\"\"Count tokens in a given text string. Replace with actual tokenizer implementation.\"\"\"\n",
    "    # This is a placeholder - replace with your actual token counting logic\n",
    "    # For example, using tiktoken for OpenAI models:\n",
    "    # import tiktoken\n",
    "    # encoding = tiktoken.get_encoding(\"cl100k_base\")  # GPT-4/GPT-3.5-turbo encoding\n",
    "    # return len(encoding.encode(text))\n",
    "    \n",
    "    # Simple word-based approximation for now (replace this)\n",
    "    return len(text.split()) * 1.3  # Rough approximation: 1.3 tokens per word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "911e7f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DOCX documents from folder\n",
    "def load_docx_documents(folder_path):\n",
    "    documents = []\n",
    "    folder = Path(folder_path)\n",
    "    \n",
    "    docx_files = list(folder.glob(\"*.docx\"))\n",
    "    \n",
    "    print(f\"Found {len(docx_files)} DOCX files to process\")\n",
    "    \n",
    "    for docx_file in docx_files:\n",
    "        try:\n",
    "            loader = Docx2txtLoader(str(docx_file))\n",
    "            doc_content = loader.load()\n",
    "            \n",
    "            for doc in doc_content:\n",
    "                doc.metadata.update({\n",
    "                    \"source\": docx_file.name,\n",
    "                    \"file_path\": str(docx_file),\n",
    "                    \"methodology\": METHODOLOGY_ID\n",
    "                })\n",
    "                documents.append(doc)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {docx_file.name}: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    print(f\"Successfully loaded {len(documents)} documents\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df98143",
   "metadata": {},
   "source": [
    "## RAG Retriever Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2906cb7d",
   "metadata": {},
   "outputs": [],
   "source": "# Vector-based retriever using OpenAI embeddings\nclass RAGRetriever(BaseRetriever):\n    \n    def __init__(self, document, methodology_id, doc_identifier):\n        super().__init__()\n        self._document = document\n        self._methodology_id = methodology_id\n        self._doc_identifier = doc_identifier\n        \n        # In-memory cache for retrieved documents\n        self._cache = {}\n        \n        store = LocalFileStore(EMBEDDING_CACHE_DIR)\n        base_embeddings = OpenAIEmbeddings()\n        self._embeddings = CacheBackedEmbeddings.from_bytes_store(\n            base_embeddings, \n            store, \n            namespace=f\"embeddings_{methodology_id}_{doc_identifier}\"\n        )\n        \n        self._text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1024,\n            chunk_overlap=128\n        )\n        \n        self.setup_vectorstore()\n    \n    def setup_vectorstore(self):\n        print(f\"  Processing document: {self._doc_identifier}\")\n        \n        splits = self._text_splitter.split_documents([self._document])\n        print(f\"  Created {len(splits)} chunks from {self._doc_identifier}\")\n        \n        self._vectorstore = InMemoryVectorStore.from_documents(splits, self._embeddings)\n    \n    def _get_relevant_documents(self, query, *, run_manager=None):\n        # Check cache first\n        if query in self._cache:\n            print(f\"  Cache hit for query: {query[:50]}...\")\n            return self._cache[query]\n        \n        # If not in cache, perform retrieval\n        print(f\"  Cache miss - retrieving for query: {query[:50]}...\")\n        documents = self._vectorstore.similarity_search(query, k=5)\n        \n        # Store in cache\n        self._cache[query] = documents\n        \n        return documents\n    \n    def clear_cache(self):\n        \"\"\"Clear the retrieval cache.\"\"\"\n        self._cache.clear()\n        print(f\"  Retrieval cache cleared for {self._doc_identifier}\")\n    \n    def get_cache_stats(self):\n        \"\"\"Get cache statistics.\"\"\"\n        return {\n            \"cache_size\": len(self._cache),\n            \"cached_queries\": list(self._cache.keys())\n        }"
  },
  {
   "cell_type": "markdown",
   "id": "4c6a3240",
   "metadata": {},
   "source": [
    "## RAG Retriever Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3quapsgpp5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing RAG Retriever for testing...\n",
      "Loaded test document: 0001_ModelDoc.docx\n",
      "Document preview: Model Documentation: 0001 - AlexNet\n",
      "\n",
      "Introduction\n",
      "\n",
      "This model documentation corresponds to AlexNet, which is a machine learning and neural network-based model for computer vision tasks.\n",
      "\n",
      "Background  \n",
      "...\n",
      "  Processing document: test_doc_0001_ModelDoc_docx\n",
      "  Created 2 chunks from test_doc_0001_ModelDoc_docx\n",
      "✅ Test retriever initialized successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tejas\\Desktop\\RAG\\.venv\\Lib\\site-packages\\langchain\\embeddings\\cache.py:58: UserWarning: Using default key encoder: SHA-1 is *not* collision-resistant. While acceptable for most cache scenarios, a motivated attacker can craft two different payloads that map to the same cache key. If that risk matters in your environment, supply a stronger encoder (e.g. SHA-256 or BLAKE2) via the `key_encoder` argument. If you change the key encoder, consider also creating a new cache, to avoid (the potential for) collisions with existing keys.\n",
      "  _warn_about_sha1_encoder()\n"
     ]
    }
   ],
   "source": [
    "# Initialize a single retriever for testing\n",
    "print(\"Initializing RAG Retriever for testing...\")\n",
    "\n",
    "# Load a single document for testing\n",
    "test_doc_path = \"./model_docs/0001_ModelDoc.docx\"\n",
    "loader = Docx2txtLoader(test_doc_path)\n",
    "test_document = loader.load()[0]\n",
    "\n",
    "# Add metadata\n",
    "test_document.metadata.update({\n",
    "    \"source\": \"0001_ModelDoc.docx\",\n",
    "    \"file_path\": test_doc_path,\n",
    "    \"methodology\": \"test\"\n",
    "})\n",
    "\n",
    "print(f\"Loaded test document: {test_document.metadata['source']}\")\n",
    "print(f\"Document preview: {test_document.page_content[:200]}...\")\n",
    "\n",
    "# Create test retriever\n",
    "test_retriever = RAGRetriever(\n",
    "    document=test_document,\n",
    "    methodology_id=\"test\",\n",
    "    doc_identifier=\"test_doc_0001_ModelDoc_docx\"\n",
    ")\n",
    "\n",
    "print(\"✅ Test retriever initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "l8dehj7psyj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing query: 'What is AlexNet and what makes it important?'\n",
      "\n",
      "Retrieving relevant documents...\n",
      "Found 2 relevant document chunks:\n",
      "\n",
      "============================================================\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Source: 0001_ModelDoc.docx\n",
      "Content (933 chars):\n",
      "Model Documentation: 0001 - AlexNet\n",
      "\n",
      "Introduction\n",
      "\n",
      "This model documentation corresponds to AlexNet, which is a machine learning and neural network-based model for computer vision tasks.\n",
      "\n",
      "Background  \n",
      "\n",
      "AlexNet is a groundbreaking convolutional neural network architecture that significantly advanced t...\n",
      "----------------------------------------\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Source: 0001_ModelDoc.docx\n",
      "Content (906 chars):\n",
      "Influence on Future Models  \n",
      "\n",
      "AlexNet inspired the development of deeper and more complex CNN architectures such as VGG, ResNet, and Inception.\n",
      "\n",
      "Practical Applications  \n",
      "\n",
      "AlexNet’s success helped enable advances in autonomous driving, facial recognition, and medical imaging.\n",
      "\n",
      "Comparison to Resnet\n",
      "\n",
      "A...\n",
      "----------------------------------------\n",
      "\n",
      "✅ Single query test completed successfully!\n",
      "Retrieved 2 chunks using invoke() method for query: 'What is AlexNet and what makes it important?'\n"
     ]
    }
   ],
   "source": [
    "# Test a single query on the retriever\n",
    "test_query = \"What is AlexNet and what makes it important?\"\n",
    "\n",
    "print(f\"Testing query: '{test_query}'\")\n",
    "print(\"\\nRetrieving relevant documents...\")\n",
    "\n",
    "# Use invoke() method following the Runnable interface\n",
    "relevant_docs = test_retriever.invoke(test_query)\n",
    "\n",
    "print(f\"Found {len(relevant_docs)} relevant document chunks:\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"\\n--- Chunk {i} ---\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'unknown')}\")\n",
    "    print(f\"Content ({len(doc.page_content)} chars):\")\n",
    "    print(doc.page_content[:300] + \"...\" if len(doc.page_content) > 300 else doc.page_content)\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(f\"\\n✅ Single query test completed successfully!\")\n",
    "print(f\"Retrieved {len(relevant_docs)} chunks using invoke() method for query: '{test_query}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ph84ml5cts9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up LCEL chain with GPT-4o-mini...\n",
      "✅ LCEL chain initialized with GPT-4o-mini\n",
      "Using CUSTOM_RAG_PROMPT (203 characters)\n",
      "\n",
      "1. Testing retrieval for: 'What is AlexNet and what makes it important?'\n",
      "============================================================\n",
      "Retrieved 2 documents:\n",
      "  1. Model Documentation: 0001 - AlexNet\n",
      "\n",
      "Introduction\n",
      "\n",
      "This model documentation corresponds to AlexNet, ...\n",
      "  2. Influence on Future Models  \n",
      "\n",
      "AlexNet inspired the development of deeper and more complex CNN archit...\n",
      "\n",
      "2. Testing LCEL chain (retrieval + generation) for: 'What is AlexNet and what makes it important?'\n",
      "============================================================\n",
      "LCEL Chain Result:\n",
      "----------------------------------------\n",
      "Question: What is AlexNet and what makes it important?\n",
      "Answer: AlexNet is a convolutional neural network architecture developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton in 2012, which significantly advanced the field of computer vision. It is important because it won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) with a top-5 error rate of 15.3%, demonstrating the effectiveness of deep learning and convolutional neural networks (CNNs) trained on large-scale datasets using GPUs. AlexNet's success popularized deep learning and inspired the development of more complex CNN architectures, leading to advancements in various practical applications such as autonomous driving, facial recognition, and medical imaging.\n",
      "Response time: 3.81s\n",
      "Model: gpt-4o-mini\n",
      "----------------------------------------\n",
      "\n",
      "✅ LCEL chain test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Test LCEL chain directly with GPT-4o-mini\n",
    "print(\"Setting up LCEL chain with GPT-4o-mini...\")\n",
    "\n",
    "# Initialize GPT-4o-mini LLM\n",
    "test_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# Create prompt template from CUSTOM_RAG_PROMPT\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=CUSTOM_RAG_PROMPT\n",
    ")\n",
    "\n",
    "# Build LCEL chain\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"context\": test_retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt_template\n",
    "    | test_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(f\"✅ LCEL chain initialized with GPT-4o-mini\")\n",
    "print(f\"Using CUSTOM_RAG_PROMPT ({len(CUSTOM_RAG_PROMPT)} characters)\")\n",
    "\n",
    "# Test retrieval only\n",
    "print(f\"\\n1. Testing retrieval for: '{test_query}'\")\n",
    "print(\"=\"*60)\n",
    "retrieved_docs = test_retriever.invoke(test_query)\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents:\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"  {i}. {doc.page_content[:100]}...\")\n",
    "\n",
    "# Test retrieval + generation with LCEL\n",
    "print(f\"\\n2. Testing LCEL chain (retrieval + generation) for: '{test_query}'\")\n",
    "print(\"=\"*60)\n",
    "start_time = time.time()\n",
    "answer = qa_chain.invoke(test_query)\n",
    "response_time = time.time() - start_time\n",
    "\n",
    "print(\"LCEL Chain Result:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Question: {test_query}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print(f\"Response time: {response_time:.2f}s\")\n",
    "print(f\"Model: gpt-4o-mini\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(f\"\\n✅ LCEL chain test completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa7f845",
   "metadata": {},
   "source": [
    "## RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d413e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main RAG pipeline using LangChain Expression Language (LCEL)\n",
    "class RAGPipeline:\n",
    "    \n",
    "    def __init__(self, retriever, llm_model, document_info, custom_prompt=None):\n",
    "        self.retriever = retriever\n",
    "        self.llm_model = llm_model\n",
    "        self.document_info = document_info\n",
    "        self.custom_prompt = custom_prompt or CUSTOM_RAG_PROMPT\n",
    "        \n",
    "        self.llm = ChatOpenAI(\n",
    "            model=llm_model,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        \n",
    "        print(f\"  RAG Pipeline initialized with {llm_model}\")\n",
    "        print(f\"  Using custom prompt: {len(self.custom_prompt)} characters\")\n",
    "        \n",
    "        # Create prompt template\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=self.custom_prompt\n",
    "        )\n",
    "        \n",
    "        # Build LCEL chain\n",
    "        self.qa_chain = (\n",
    "            {\n",
    "                \"context\": self.retriever | format_docs,\n",
    "                \"question\": RunnablePassthrough(),\n",
    "            }\n",
    "            | prompt_template\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        print(f\"  Using LangChain Expression Language (LCEL) chain\")\n",
    "    \n",
    "    def query(self, question):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Get documents for metadata/sources tracking\n",
    "            docs = self.retriever.invoke(question)\n",
    "            \n",
    "            # Format context for prompt token counting\n",
    "            formatted_context = format_docs(docs)\n",
    "            \n",
    "            # Build the final prompt that will be sent to LLM and count tokens\n",
    "            final_prompt = self.custom_prompt.format(\n",
    "                context=formatted_context,\n",
    "                question=question\n",
    "            )\n",
    "            prompt_tokens = get_token_count(final_prompt)\n",
    "            \n",
    "            # Use LCEL chain to get answer\n",
    "            answer = self.qa_chain.invoke(question)\n",
    "            \n",
    "            # Count output tokens\n",
    "            output_tokens = get_token_count(answer)\n",
    "            \n",
    "            # Calculate total tokens (standard billing calculation)\n",
    "            total_tokens = prompt_tokens + output_tokens\n",
    "            \n",
    "            sources = []\n",
    "            for doc in docs:\n",
    "                sources.append({\n",
    "                    \"content\": doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content,\n",
    "                    \"source\": doc.metadata.get(\"source\", \"unknown\"),\n",
    "                    \"metadata\": doc.metadata\n",
    "                })\n",
    "            \n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "            final_result = {\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"sources\": sources,\n",
    "                \"num_sources\": len(sources),\n",
    "                \"response_time\": response_time,\n",
    "                \"llm_model\": self.llm_model,\n",
    "                \"methodology\": METHODOLOGY_ID,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"document_name\": self.document_info[\"name\"],\n",
    "                \"document_source\": self.document_info[\"source\"],\n",
    "                \"document_id\": self.document_info[\"id\"],\n",
    "                \"prompt_length\": len(self.custom_prompt),\n",
    "                \"retriever_type\": type(self.retriever).__name__,\n",
    "                \"prompt_tokens\": prompt_tokens,\n",
    "                \"output_tokens\": output_tokens,\n",
    "                \"total_tokens\": total_tokens\n",
    "            }\n",
    "            \n",
    "            return final_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"answer\": f\"Error: {str(e)}\",\n",
    "                \"sources\": [],\n",
    "                \"response_time\": time.time() - start_time,\n",
    "                \"llm_model\": self.llm_model,\n",
    "                \"methodology\": METHODOLOGY_ID,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"num_sources\": 0,\n",
    "                \"document_name\": self.document_info[\"name\"],\n",
    "                \"document_source\": self.document_info[\"source\"],\n",
    "                \"document_id\": self.document_info[\"id\"],\n",
    "                \"retriever_type\": type(self.retriever).__name__,\n",
    "                \"error\": str(e),\n",
    "                \"prompt_tokens\": 0,\n",
    "                \"output_tokens\": 0,\n",
    "                \"total_tokens\": 0\n",
    "            }\n",
    "    \n",
    "    def batch_query(self, questions):\n",
    "        results = []\n",
    "        \n",
    "        for i, question in enumerate(questions, 1):\n",
    "            print(f\"    Question {i}/{len(questions)}: {question[:50]}...\")\n",
    "            result = self.query(question)\n",
    "            results.append(result)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e5b7e",
   "metadata": {},
   "source": [
    "## Multi-Run Execution Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "278b135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments across multiple documents and models\n",
    "def run_multiple_experiments(questions, documents, llm_models, n_runs):\n",
    "    if isinstance(llm_models, str):\n",
    "        llm_models = [llm_models]\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    total_executions = len(questions) * len(documents) * len(llm_models) * n_runs\n",
    "    print(f\"Starting experiments:\")\n",
    "    print(f\"  - {len(documents)} documents\")\n",
    "    print(f\"  - {len(questions)} questions per document\")\n",
    "    print(f\"  - {len(llm_models)} models\")\n",
    "    print(f\"  - {n_runs} runs per question\")\n",
    "    print(f\"  - Total executions: {total_executions}\")\n",
    "    \n",
    "    current_execution = 0\n",
    "    \n",
    "    for doc_idx, document in enumerate(documents, 1):\n",
    "        doc_name = document.metadata.get('source', f'doc_{doc_idx}')\n",
    "        doc_identifier = f\"doc_{doc_idx}_{doc_name.replace('.', '_').replace(' ', '_')}\"\n",
    "        \n",
    "        print(f\"\\n=== Document {doc_idx}/{len(documents)}: {doc_name} ===\")\n",
    "        \n",
    "        document_info = {\n",
    "            \"name\": doc_name,\n",
    "            \"source\": document.metadata.get('source', 'unknown'),\n",
    "            \"id\": doc_identifier\n",
    "        }\n",
    "        \n",
    "        print(f\"Setting up retriever for {doc_name}...\")\n",
    "        doc_retriever = RAGRetriever(document, METHODOLOGY_ID, doc_identifier)\n",
    "        \n",
    "        for model_idx, model in enumerate(llm_models, 1):\n",
    "            print(f\"\\n--- Model {model_idx}/{len(llm_models)}: {model} ---\")\n",
    "            \n",
    "            pipeline = RAGPipeline(doc_retriever, model, document_info, CUSTOM_RAG_PROMPT)\n",
    "            \n",
    "            for run_idx in range(1, n_runs + 1):\n",
    "                print(f\"\\n  Run {run_idx}/{n_runs} on {doc_name}\")\n",
    "                \n",
    "                run_results = pipeline.batch_query(questions)\n",
    "                \n",
    "                for result in run_results:\n",
    "                    result[\"run_number\"] = run_idx\n",
    "                    result[\"model_index\"] = model_idx\n",
    "                    result[\"document_index\"] = doc_idx\n",
    "                    all_results.append(result)\n",
    "                    \n",
    "                    current_execution += 1\n",
    "                    if current_execution % 10 == 0:\n",
    "                        print(f\"    Progress: {current_execution}/{total_executions} ({100*current_execution/total_executions:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nCompleted! Total results: {len(all_results)}\")\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c822188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save experiment results to CSV\n",
    "def save_results_to_csv(results, output_dir):\n",
    "    df_data = []\n",
    "    \n",
    "    for result in results:\n",
    "        row = {\n",
    "            \"methodology\": result[\"methodology\"],\n",
    "            \"llm_model\": result[\"llm_model\"],\n",
    "            \"document_name\": result[\"document_name\"],\n",
    "            \"document_source\": result[\"document_source\"],\n",
    "            \"document_id\": result[\"document_id\"],\n",
    "            \"run_number\": result[\"run_number\"],\n",
    "            \"question\": result[\"question\"],\n",
    "            \"answer\": result[\"answer\"],\n",
    "            \"response_time\": result[\"response_time\"],\n",
    "            \"num_sources\": result[\"num_sources\"],\n",
    "            \"timestamp\": result[\"timestamp\"],\n",
    "            \"sources_summary\": \" | \".join([s[\"source\"] for s in result[\"sources\"]]) if result[\"sources\"] else \"\",\n",
    "            \"prompt_length\": result.get(\"prompt_length\", 0),\n",
    "            \"retriever_type\": result.get(\"retriever_type\", \"unknown\"),\n",
    "            \"error\": result.get(\"error\", \"\"),\n",
    "            # Token count columns\n",
    "            \"prompt_tokens\": result.get(\"prompt_tokens\", 0),\n",
    "            \"output_tokens\": result.get(\"output_tokens\", 0),\n",
    "            \"total_tokens\": result.get(\"total_tokens\", 0)\n",
    "        }\n",
    "        df_data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(df_data)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{METHODOLOGY_ID}_results_{timestamp}.csv\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    df.to_csv(filepath, index=False)\n",
    "    \n",
    "    print(f\"Results saved to: {filepath}\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(f\"Unique documents: {df['document_name'].nunique()}\")\n",
    "    print(f\"Questions per document: {len(df) // df['document_name'].nunique() if df['document_name'].nunique() > 0 else 0}\")\n",
    "    print(f\"Retriever types used: {df['retriever_type'].unique().tolist()}\")\n",
    "    \n",
    "    # Token usage summary\n",
    "    if 'total_tokens' in df.columns and df['total_tokens'].sum() > 0:\n",
    "        print(f\"\\nToken Usage Summary:\")\n",
    "        print(f\"Total tokens across all queries: {df['total_tokens'].sum():,}\")\n",
    "        print(f\"Average tokens per query: {df['total_tokens'].mean():.1f}\")\n",
    "        print(f\"Average prompt tokens: {df['prompt_tokens'].mean():.1f}\")\n",
    "        print(f\"Average output tokens: {df['output_tokens'].mean():.1f}\")\n",
    "    \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a698a6a3",
   "metadata": {},
   "source": [
    "## Load Documents and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "396bf167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from: ./model_docs\n",
      "Found 1 DOCX files to process\n",
      "Successfully loaded 1 documents\n",
      "Successfully loaded 1 documents\n",
      "\n",
      "Document overview:\n",
      "  1. 0001_ModelDoc.docx: Model Documentation: 0001 - AlexNet\n",
      "\n",
      "Introduction\n",
      "\n",
      "This model documentation corresponds to AlexNet, ...\n"
     ]
    }
   ],
   "source": [
    "# Load documents from configured folder\n",
    "print(f\"Loading documents from: {DOCX_FOLDER_PATH}\")\n",
    "documents = load_docx_documents(DOCX_FOLDER_PATH)\n",
    "\n",
    "if not documents:\n",
    "    print(\"No documents loaded. Please check the folder path or add some DOCX files.\")\n",
    "else:\n",
    "    print(f\"Successfully loaded {len(documents)} documents\")\n",
    "    \n",
    "    print(\"\\nDocument overview:\")\n",
    "    for i, doc in enumerate(documents[:3]):\n",
    "        source = doc.metadata.get('source', 'unknown')\n",
    "        content_preview = doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content\n",
    "        print(f\"  {i+1}. {source}: {content_preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "411ec035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents ready for individual processing: 1 documents loaded\n",
      "Each document will be processed separately with its own retriever and vector store\n"
     ]
    }
   ],
   "source": [
    "# Confirm documents are ready for processing\n",
    "if documents:\n",
    "    print(f\"Documents ready for individual processing: {len(documents)} documents loaded\")\n",
    "    print(\"Each document will be processed separately with its own retriever and vector store\")\n",
    "else:\n",
    "    print(\"Cannot proceed without documents\")\n",
    "    documents = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f80daa",
   "metadata": {},
   "source": [
    "## Question Set Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d47e5fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 2 questions from ./example_questions.txt\n",
      "\n",
      "Loaded Questions (2 total):\n",
      "   1. What does this model cover?\n",
      "   2. Why is this model important?\n"
     ]
    }
   ],
   "source": [
    "# Load questions from text file\n",
    "def load_questions_from_file(file_path):\n",
    "    questions = []\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line_num, line in enumerate(file, 1):\n",
    "                line = line.strip()\n",
    "                \n",
    "                if not line or line.startswith('#'):\n",
    "                    continue\n",
    "                \n",
    "                questions.append(line)\n",
    "                \n",
    "        print(f\"Successfully loaded {len(questions)} questions from {file_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading questions file {file_path}: {e}\")\n",
    "        raise e\n",
    "        \n",
    "    return questions\n",
    "\n",
    "TEST_QUESTIONS = load_questions_from_file(QUESTIONS_FILE_PATH)\n",
    "\n",
    "print(f\"\\nLoaded Questions ({len(TEST_QUESTIONS)} total):\")\n",
    "for i, question in enumerate(TEST_QUESTIONS, 1):\n",
    "    display_question = question if len(question) <= 60 else question[:57] + \"...\"\n",
    "    print(f\"  {i:2d}. {display_question}\")\n",
    "\n",
    "if len(TEST_QUESTIONS) > 10:\n",
    "    print(f\"  ... and {len(TEST_QUESTIONS) - 10} more questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80656cb5",
   "metadata": {},
   "source": [
    "## Execute RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebfd0e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting document-by-document RAG pipeline execution...\n",
      "Configuration:\n",
      "  - Methodology: semantic\n",
      "  - Models: ['gpt-4o']\n",
      "  - Documents: 1\n",
      "  - Questions per document: 2\n",
      "  - Runs per question: 3\n",
      "Starting experiments:\n",
      "  - 1 documents\n",
      "  - 2 questions per document\n",
      "  - 1 models\n",
      "  - 3 runs per question\n",
      "  - Total executions: 6\n",
      "\n",
      "=== Document 1/1: 0001_ModelDoc.docx ===\n",
      "Setting up retriever for 0001_ModelDoc.docx...\n",
      "  Processing document: doc_1_0001_ModelDoc_docx\n",
      "  Created 2 chunks from doc_1_0001_ModelDoc_docx\n",
      "\n",
      "--- Model 1/1: gpt-4o ---\n",
      "  RAG Pipeline initialized with gpt-4o\n",
      "  Using custom prompt: 203 characters\n",
      "  Using LangChain Expression Language (LCEL) chain\n",
      "\n",
      "  Run 1/3 on 0001_ModelDoc.docx\n",
      "    Question 1/2: What does this model cover?...\n",
      "    Question 2/2: Why is this model important?...\n",
      "\n",
      "  Run 2/3 on 0001_ModelDoc.docx\n",
      "    Question 1/2: What does this model cover?...\n",
      "    Question 2/2: Why is this model important?...\n",
      "\n",
      "  Run 3/3 on 0001_ModelDoc.docx\n",
      "    Question 1/2: What does this model cover?...\n",
      "    Question 2/2: Why is this model important?...\n",
      "\n",
      "Completed! Total results: 6\n",
      "Results saved to: ./outputs/semantic\\semantic_results_20250730_100808.csv\n",
      "Total rows: 6\n",
      "Unique documents: 1\n",
      "Questions per document: 6\n",
      "Retriever types used: ['RAGRetriever']\n",
      "\n",
      "✅ Pipeline execution complete!\n",
      "Results saved to: ./outputs/semantic\\semantic_results_20250730_100808.csv\n"
     ]
    }
   ],
   "source": [
    "# Execute RAG pipeline experiments\n",
    "if documents and TEST_QUESTIONS:\n",
    "    print(\"Starting document-by-document RAG pipeline execution...\")\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  - Methodology: {METHODOLOGY_ID}\")\n",
    "    print(f\"  - Models: {LLM_MODELS}\")\n",
    "    print(f\"  - Documents: {len(documents)}\")\n",
    "    print(f\"  - Questions per document: {len(TEST_QUESTIONS)}\")\n",
    "    print(f\"  - Runs per question: {N_RUNS}\")\n",
    "    \n",
    "    all_results = run_multiple_experiments(\n",
    "        questions=TEST_QUESTIONS,\n",
    "        documents=documents,\n",
    "        llm_models=LLM_MODELS,\n",
    "        n_runs=N_RUNS\n",
    "    )\n",
    "    \n",
    "    csv_path = save_results_to_csv(all_results, OUTPUT_DIR)\n",
    "    \n",
    "    print(f\"\\n✅ Pipeline execution complete!\")\n",
    "    print(f\"Results saved to: {csv_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot execute pipeline - missing documents or questions\")\n",
    "    all_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b48a82",
   "metadata": {},
   "source": [
    "## Results Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e377b19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RESULTS PREVIEW ===\n",
      "\n",
      "Summary Statistics:\n",
      "Total results: 6\n",
      "Unique documents processed: 1\n",
      "Average response time: 1.71 seconds\n",
      "Average sources per answer: 2.0\n",
      "\n",
      "Results by Model:\n",
      "          response_time       num_sources\n",
      "                   mean   std        mean\n",
      "llm_model                                \n",
      "gpt-4o             1.71  0.64         2.0\n",
      "\n",
      "Results by Document:\n",
      "                    response_time  num_sources\n",
      "document_name                                 \n",
      "0001_ModelDoc.docx           1.71          2.0\n",
      "\n",
      "Sample Results:\n",
      "\n",
      "--- Result 1 ---\n",
      "Document: 0001_ModelDoc.docx\n",
      "Model: gpt-4o\n",
      "Run: 1\n",
      "Question: What does this model cover?\n",
      "Answer: This model documentation covers AlexNet, a convolutional neural network architecture used for computer vision tasks, particularly focusing on its deve...\n",
      "Sources: 2\n",
      "Time: 1.57s\n",
      "\n",
      "--- Result 2 ---\n",
      "Document: 0001_ModelDoc.docx\n",
      "Model: gpt-4o\n",
      "Run: 1\n",
      "Question: Why is this model important?\n",
      "Answer: AlexNet is important because it significantly advanced the field of computer vision by popularizing deep learning and demonstrating the power of convo...\n",
      "Sources: 2\n",
      "Time: 2.25s\n",
      "\n",
      "--- Result 3 ---\n",
      "Document: 0001_ModelDoc.docx\n",
      "Model: gpt-4o\n",
      "Run: 2\n",
      "Question: What does this model cover?\n",
      "Answer: The model documentation covers AlexNet, a convolutional neural network architecture for computer vision tasks....\n",
      "Sources: 2\n",
      "Time: 1.02s\n"
     ]
    }
   ],
   "source": [
    "# Display results summary and preview\n",
    "if all_results:\n",
    "    print(\"=== RESULTS PREVIEW ===\")\n",
    "    \n",
    "    df = pd.DataFrame([{\n",
    "        \"methodology\": r[\"methodology\"],\n",
    "        \"llm_model\": r[\"llm_model\"],\n",
    "        \"document_name\": r[\"document_name\"],\n",
    "        \"run_number\": r[\"run_number\"],\n",
    "        \"response_time\": r[\"response_time\"],\n",
    "        \"num_sources\": r[\"num_sources\"]\n",
    "    } for r in all_results])\n",
    "    \n",
    "    print(f\"\\nSummary Statistics:\")\n",
    "    print(f\"Total results: {len(all_results)}\")\n",
    "    print(f\"Unique documents processed: {df['document_name'].nunique()}\")\n",
    "    print(f\"Average response time: {df['response_time'].mean():.2f} seconds\")\n",
    "    print(f\"Average sources per answer: {df['num_sources'].mean():.1f}\")\n",
    "    \n",
    "    print(f\"\\nResults by Model:\")\n",
    "    model_summary = df.groupby('llm_model').agg({\n",
    "        'response_time': ['mean', 'std'],\n",
    "        'num_sources': 'mean'\n",
    "    }).round(2)\n",
    "    print(model_summary)\n",
    "    \n",
    "    print(f\"\\nResults by Document:\")\n",
    "    doc_summary = df.groupby('document_name').agg({\n",
    "        'response_time': 'mean',\n",
    "        'num_sources': 'mean'\n",
    "    }).round(2)\n",
    "    print(doc_summary)\n",
    "    \n",
    "    print(f\"\\nSample Results:\")\n",
    "    for i, result in enumerate(all_results[:3]):\n",
    "        print(f\"\\n--- Result {i+1} ---\")\n",
    "        print(f\"Document: {result['document_name']}\")\n",
    "        print(f\"Model: {result['llm_model']}\")\n",
    "        print(f\"Run: {result['run_number']}\")\n",
    "        print(f\"Question: {result['question']}\")\n",
    "        print(f\"Answer: {result['answer'][:150]}...\")\n",
    "        print(f\"Sources: {result['num_sources']}\")\n",
    "        print(f\"Time: {result['response_time']:.2f}s\")\n",
    "\n",
    "else:\n",
    "    print(\"No results to display\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}